# Tokenizer

일본어 Byte-level BPE tokenizer를 학습하면서 삽질한 후기

일단 일본어의 경우 띄어쓰기가 존재하지 않는다. 따라서 띄어쓰기가 존재하지않는 언어에 대해 모두 해당될 것이라고 생각한다.

이와 같은 언어는 다량의 데이터로 Byte-level BPE를 학습하게 된다면 OOM이 발생한다.

그 이유는 BBPE 알고리즘에 의해 한 문장을 하나의 토큰으로 설정하고 들어가므로 문장이 많을수록 메모리를 잡아먹기 때문이다.

또한, 한 문장이 마침표 및 느낌표와 같은 split 마크 마다 split이 되어 있지 않다면, 한 문장안에 여러문장이 포함되어 있을 것이고,

Unique한 문장 또한 많아질 가능성이 있다. (꼭 그런것 만은 아니다)

어쨌든, 한 문장을 하나의 토큰으로 보기 때문에 OOM이 발생하게 되는데, 이때 해결방법으로는 tokenizer를 학습할 데이터를 줄이던지,

학습 하기 전 문장을 Mecab과 같은 다른 tokenizer를 이용하여 pre-tokenizing 하여 띄어쓰기를 만들어 놓아야 한다.

띄어쓰기가 들어가게 되면 단어 별로 토큰 처리를 하므로 중복 단어가 생겨 메모리를 많이 잡아 먹지 않게 되어 해결 가능하다.

